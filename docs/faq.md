# Памятка для пользователей кластера "Макарьич"

## Как получить аккаунт на кластере?

Чтобы получить аккаунт на кластере, необходимо заполнить [форму регистрации](http://makarich.fbb.msu.ru/signup.html).

В случае возникновения вопросов, можно персонально связаться с администраторами:

- Залевский Артур (aozalevsky@fbb.msu.ru)
- Томаровский Андрей (andrey.tomarovsky@gmail.com)

или руководителем Лаборатории:

- Егор Базыкин (gbazykin@iitp.ru)

## Как зайти на кластер?

Доступ к кластер осуществляется по протоколу SSH.
### GNU/Linux, BSD и OS X:

Пользователи указанных операционных систем могут воспользоваться командой ssh:

	ssh username@ma.fbb.msu.ru

### Windows

Пользователи Windows могут воспользоваться одной из следующих программ: Putty, tunnelier.

При этом необходимо указать следующие параметры:

	Адрес: ma.fbb.msu.ru
	Протокол: ssh
	Порт: 22

### Смена пароля

При регистрации аккаунта администраторы назначают временный пароль. Желательно его сменить сразу же, после первого успешного логина на кластер. Сделать это можно при помощи команды `passwd`

## Как скопировать файлы?

Для копирования файлов рекомендуется использовать протоколы scp/sftp.

### GNU/Linux, BSD и OS X:

Пользователи перечисленных операционных систем могут воспользоваться следующими командами:

Туда: `scp path/to/source/file username@ma.fbb.msu.ru:path/to/destination/`

Обратно: `scp username@ma.fbb.msu.ru:path/to/source/file path/to/destination/`

Чтобы копировать директории, необходимо воспользоваться ключом -r (от recursive): `scp -r path/to/folder username@ma.fbb.msu.ru:path/to/destination/`

Можно использовать относительные пути (path/to/source или абсолютные /path/to/source). Для определения абсолютного пути удобно использовать команду `readlink -f path/to/source`

### Windows

Пользователи Windows могут воспользоваться следующими приложениями: WinSCP или WinSCP as FAR Plugin

Параметры для подключения такие же, как в секции про доступ к кластеру.

## Расчеты на кластере

**Не считайте на главном узле** (главный узел - это head02, куда вы попадаете, когда заходите)
Для запуска задачи нужно создать стартовый скрипт (например, script.sh)


	#!/bin/bash
	#PBS -d .
	#PBS -l walltime=100:00:00,mem=10gb
	wc -l moscow.fca.fastq.filtered > moscow.fca.fastq.filtered.wc


В скрипте указано, что требуется 100 часов расчетного времени и 10Г памяти. По умолчанию расчетное время (walltime) равно 100 часам. Если не указать walltime, то через час задача будет автоматически завершена. Более подробно о запросе ресурсов можно прочитать в файле man pbs_resources
Например, чтобы использовать 10 ядер, нужно задать `#PBS -l nodes=1:ncpus=10`

### Conda

Для использования Conda окружений необходимо указать `#!/bin/bash -l`. Таким образом:

	#!/bin/bash -l
	#PBS -d .
	#PBS -l nodes=1:ncpus=10
	#PBS -l walltime=100:00:00,mem=10gb
	conda actibate mybase
	tool_name -i input >> output

Для запуска скрипта на языке программирования R необходимо сперва подгрузить модуль с нужной версией языка, а после запустить скрипт при помощи команды `Rscript <script.r>`

Поставить задачу на выполнение: `qsub script.sh`
В ходе выполнения задачи создаются 2 файла `start.sh.o<номерзадачи>` - поток вывода и `start.sh.e<номерзадачи>` - поток ошибок.

Если задача создает очень большой (гигабайты) файл вывода, то лучше перенаправлять его сразу в определенный файл, например:

`perl script.pl > /home/user/result.out`

Иначе, весь большой вывод сначала создается в виде файла shart.sh.0 на одном из расчетных узлов, а в конце счета копируется по сети в домашнюю папку, что замедляет работу главного узла.

Просмотреть состояние очереди `qstat -n1` - подробно, `qstat` - кратко.

Запускать на короткое время и отлаживать программы можно с любого свободного от задач узла. Зайти на узел можно с основного по `ssh nodeX`

Посмотреть доступные узлы можно командой `qhost`, более подробно - `pbsnodes`.
`pbsnodes -l free` - показывает все узлы, на которых свободно хотя бы одно ядро.

`checkjob -v` покажет статус задачи. Если задача ждет в очереди, то указывается также почему задача не попала на каждый из узлов. Например узел может быть полностью загружен, нет запрошенного количества ядер или памяти.
Массивы задач

Когда нужно обработать много файлов, например, пробластовать их против базы, хорошо использовать массивы задач. Не делайте массивов больше чем на 1000 задач.
Например:

	#!/bin/bash
	#PBS -d .

	echo $PBS_ARRAYID

Запуск задачи: `qsub -t 0-39 start.sh`. Просмотр очереди: `qstat -t`
Порождается 40 одинаковых задач, каждой выделяется свое ядро, каждой передается свое значение переменной `$PBS_ARRAYID` от 0 до 39. В зависимости от значения, можно внутри скрипта обратиться к какому-либо файлу, например `perl transl.pl $PBS_ARRAYID.fasta > $PBS_ARRAYID.prot`.
Можно ограничить ваш массив одним узлом: `#PBS -l nodes=node13`, тогда начнется выполнение первых 24-х задач, а остальные будут ждать в очереди.
Если указать строку `#PBS -l nodes=node13:ppn=20+node14:ppn=20` для массива задач, то эта строка работает не для всего массива, а для каждой задачи из массива. Т.е. получается, что каждая из 100 задач будет пытаться занять 20 ядер на узле 13 и 20 ядер на узле 14. Следовательно, все считаться они будут медленно, и 39 из занятых 40 ядер будут простаивать. Не делайте так.
Типы доступных узлов (очереди)

| name     | cpu | mem  | node count | comment         |
|----------|-----|------|------------|-----------------|
| eternity | 24  | 48G  | 30         | default         |
| big      | 32  | 128G | 3          |                 |
| mem512g  | 48  | 512G | 1          |                 |
| mem1t    | 80  | 1T   | 1          |                 |
| mem1t    | 80  | 2T   | 1          |                 |
| gpu      | 24  | 48G  | 2          | Nvidia GTX 1060 |

Для запуска задачи в определенную очередь, необходимо выполнить команду
`qsub -q queue_name scriptname`
Например:
`qsub -q big -l nodes=1 script.sh`

По умолчанию задачи попадают в очередь `eternity`

Для остановки/удаления задачи из очереди можно воспользоваться командой qdel:
`qdel 139843.head02`

Для просмотра детальной информации о задачи можно использовать команду qstat:
`qstat -f 139843.head02`

### Ограничения по вводу-выводу

Все вычислительные узлы, а также
рабочий узел head02 подключены к файл-серверу (/home). Если файл сервер
начинает испытывать очень большие нагрузки по чтению-записи, то
становится трудно работать на head02.

Не запускайте одновременно много задач интенсивно использующие I/O. Задачи будут тормозить значительно дольше, чем если бы они были бы запущены по очереди.

По умолчанию программы вроде tar, cp запускаются с ionice -3. К сожалению это нельзя сделать для cat. Если вам нужно слить два огромных файла или что-то еще с помощью cat, то лучше используйте pv. Это интерактивный аналог cat, который может показывать progress-bar, скорость передачи данных, и самое главное, ограничивать скорость передачи данных подобно rsync. Пример использования: `pv -L 10m file1 file2 > concatenated`

### Сборка геномов/транскриптомов

Для операций над большим количеством файлов, на узлах node01,node02 и node36,node33-node35 созданы специальные scratch разделы /mnt/scr размером 600Gb, 1Tb, 2Tb соответственно. Если вам необходимо, например, собрать транскриптом, то имеет смысл использовать именно их - это существенно поднимет скорость выполнения задачи.

Типичный PBS скрипт с использованием scratch директории будет выглядеть так:

	#!/bin/bash
	#PBS -d .

	# Copy data from main storage to scratch
	cp /home/my/data /mnt/scr/data
	cd /mnt/scr/data

	# Do some job
	do_some_job

	# Copy results back to main storage
	cp /mnt/scr/data/results /home/my/data

	# Clean all stuff
	rm -rf /mnt/scr/data

NB: Будьте внимательны, в случае Array job

### Доступное ПО

Многие программы, необходимые для работы с геномными данными уже установлены на кластере - их можно найти в директории /home/tools. Если вам нужна какая-то программа, и её нет в /home/tools, но она может пригодиться другим, то установите её в /home/tools/newprogram или попросите об этом администраторов.
Модули с переменными окружения

Некоторое ПО доступно при помощи так называемой системы модулей, в частности последние версии интерпретаторов Python и некоторое другое специфичное ПО. Про использование модулей можно прочитать тут.

Список доступных модулей можно получить при помощи команды `module avail`

По умолчанию стоит ограничение на занимаемое количество памяти:

`_JAVA_OPTIONS="-Xmx3g"`

Если нужно больше, надо подумать действительно ли задача требует много памяти, и затем задать другое значение.

`export _JAVA_OPTIONS=...`

### Удаленный графический доступ

Мы запустили в тестовом режиме возможно удаленно работать с графическими приложениями. Все действия осуществляются на специально выделенном узле. В данный момент, по соображениям безопасности, доступ возможен только из сети Факультета.

Для доступа необходимо:
- Cкачать и установить клиент x2go под соответствующую ОС (поддерживаются GNU/Linux, Windows, Mac OS X)

- Согласно инструкции (вариант с картинками) создать новую сессию, где указать следующие параметры:

	Host: ma.fbb.msu.ru
	Login: your_username
	SSH port: 222
	Session type: GNOME

- Попробовать подключиться - все должно работать. Если вы используете для авторизации ключ, его также можно указать при создании сесии. На вкладке Connection можно выбрать тип вашего соединения, для оптимизации производительности. На вкладке Shared folders можно дополнительно подключить локальные директории (на вашем компьютере), которые вам будут также доступны на кластере. Соответственно можно легко копировать файлы, но этот интерфейс далеко не самый производительный. При отключении сессии сохраняются

### IGV

Одна из ключевых возможностей, ради которой мы реализовали удаленный графический доступ - возможность работы с геномными браузерами.

Для использования IGV необходимо открыть терминал и ввести следующие команды

	module load igv
	igv.sh

Первая активирует соответствующий модуль и устанавливает необходимые пути, вторая непосредственно запускает IGV.