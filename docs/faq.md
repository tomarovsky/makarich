# Памятка для пользователей кластера "Макарьич"

## Как получить аккаунт на кластере?

Чтобы получить аккаунт на кластере, необходимо заполнить [форму регистрации](http://makarich.fbb.msu.ru/signup.html).

В случае возникновения вопросов, можно персонально связаться с администраторами:

- Залевский Артур (aozalevsky@fbb.msu.ru)
- Томаровский Андрей (andrey.tomarovsky@gmail.com)

или руководителем Лаборатории:

- Егор Базыкин (gbazykin@iitp.ru)

## Как зайти на кластер?

Доступ к кластеру осуществляется по протоколу SSH.

### GNU/Linux, BSD и OS X:

Пользователи указанных операционных систем могут воспользоваться командой ssh:

	ssh username@ma.fbb.msu.ru

### Windows

Пользователи Windows могут воспользоваться одной из следующих программ: **Putty**, **tunnelier**.

При этом необходимо указать следующие параметры:

	Адрес: ma.fbb.msu.ru
	Протокол: ssh
	Порт: 22

Альтертанивным решением будет включение WSL (Windows Subsystem for Linux), подробно описанное [здесь](https://docs.microsoft.com/ru-ru/windows/wsl/install-win10#manual-installation-steps).

### Смена пароля

При регистрации аккаунта администраторы назначают временный пароль. Желательно его сменить сразу же, после первого успешного логина на кластер. Сделать это можно при помощи команды `passwd`

## Как скопировать файлы?

Для копирования файлов рекомендуется использовать протоколы scp/sftp.

### GNU/Linux, BSD и OS X:

Пользователи перечисленных операционных систем могут воспользоваться следующими командами:

С ПК на кластер: `scp path/to/source/file username@ma.fbb.msu.ru:path/to/destination/`

С кластера на ПК: `scp username@ma.fbb.msu.ru:path/to/source/file path/to/destination/`

Чтобы копировать директории, необходимо воспользоваться ключом -r (от _recursive_): `scp -r path/to/folder username@ma.fbb.msu.ru:path/to/destination/`

Можно использовать относительные пути (path/to/source или абсолютные /path/to/source). Для определения абсолютного пути удобно использовать команду `readlink -f path/to/source`

### Windows

Пользователи Windows могут воспользоваться следующими приложениями: WinSCP или WinSCP as FAR Plugin

Параметры для подключения такие же, как в секции про доступ к кластеру.

## Расчеты на кластере

**Не считайте на главном узле** (главный узел - это _head02_, куда вы попадаете, когда заходите)
На кластере используется Portable Batch System (PBS). Для запуска задачи нужно создать стартовый скрипт (например, _script.sh_, который считает количество прочтений в файле FASTQ и записывает результат в файл):

	#!/bin/bash
	#PBS -d .
	#PBS -l nodes=1:ppn=5
	#PBS -l walltime=100:00:00,mem=10gb
	echo $(zcat sample.fastq.gz | wc -l)/4 | bc > sample.fastq.wc

В скрипте указано, что задача будет:

- выполняться на 1-ом узеле (nodes=1);
- использовать 5 ядер (ppn=5) и 10Гб памяти (mem=10gb);
- выполняться максимум 100 часов расчетного времени (walltime=100:00:00). Если задача не будет выполнена по истечению заданного срока, она будет принудительно завершена.

По умолчанию расчетное время (walltime) равно 100 часам. Если не указать walltime, то через час задача будет автоматически завершена. Более подробно о запросе ресурсов можно прочитать в файле man pbs_resources, либо на официальном сайте.

### Использование модулей

На клатере реализована так называемая "система модулей". С ее помощью можно быстро получать доступ к различному ПО и к последним версиям интерпретаторов Python, R и т.д.

Основные команды:

- Посмотреть список доступных модулей - `module avail`;
- Подгрузить модуль - `module load <modulename>`;
- Отключить модуль - `module unload <modulename>`;
- Список активированных на данный момент модулей - `module list`.

Для примера, на кластере установлено несколько версий Python, удобное переключение между которыми реализовано через модули:

	module load python/python-3.7.4
	python3 --version  # Python 3.7.4
	module unload python/python-3.7.4
	module load python/python-3.8.2
	python3 --version  # Python 3.8.2

Для запуска скрипта на языке программирования R необходимо подгрузить модуль с нужной версией языка, а после запустить скрипт при помощи команды `Rscript <script.r>`. Запуск скриптов на языке Perl осуществляется командой `perl <script.pl>`, также с предварительной подгрузкой модуля языка.

### Conda

Актуальную версию Miniconda можно загрузить с официального [сайта](https://docs.conda.io/en/latest/miniconda.html). После установки рекомендуется отключить автоматическую активацию _base_-окружения командой:

	conda config --set auto_activate_base false

Для установки необходимой программы необходимо создать окружение, активировать его и выполнить установку. Не рекомендуется устанавливать все инструменты в одно окружение, т.к. это повышает вероятность конфликта зависимостей.

- Создать новое окружение: `conda create -n mybase`;
- Удалить окружение: `conda env remove --name mybase`;
- Просмотр созданных окружений - `conda env list`;
- Активация окружения: `conda activate mybase`;
- Деактивация активного окружения: `conda deactivate`;
- Для установки нужной программы (к примеру _parallel_): `conda install -c conda-forge parallel`.
Через параметр -c указывается канал (от _channel_), обычно это conda-forge, либо bioconda, но лучше узнать индивидуально для каждого устанавливаемого инструмента.

Для использования Conda окружений в PBS-скриптах необходимо указать `#!/bin/bash -l`:

	#!/bin/bash -l
	#PBS -d .
	#PBS -l nodes=1:ppn=5
	#PBS -l walltime=100:00:00,mem=10gb
	conda actibate mybase
	tool_name -i input >> output

### Очередь задач

- Просмотреть состояние очереди `qstat -n1` - подробно, `qstat` - кратко. Запускать на короткое время и отлаживать программы можно с любого свободного от задач узла. Зайти на узел можно с основного по `ssh nodeX`.

- Посмотреть доступные узлы можно командой `qhost`, более подробно - `pbsnodes` (все узлы, на которых свободно хотя бы одно ядро - `pbsnodes -l free`).
- Поставить задачу на выполнение: `qsub script.sh`. В ходе выполнения задачи создаются 2 файла `start.sh.o<номерзадачи>` - поток вывода и `start.sh.e<номерзадачи>` - поток ошибок. 

Если задача создает очень большой (гигабайты) файл вывода, то лучше перенаправлять его сразу в определенный файл, например:

	perl script.pl > /home/user/result.out

Иначе, весь большой вывод сначала создается в виде файла _shart.sh.0_ на одном из расчетных узлов, а в конце счета копируется по сети в домашнюю папку, что замедляет работу главного узла.

- Узнать статус задачи: `checkjob -v`. Если задача ждет в очереди, то указывается также почему задача не попала на каждый из узлов. Например, узел может быть полностью загружен, нет запрошенного количества ядер или памяти.

- Для запуска задачи в определенную очередь, необходимо выполнить команду
`qsub -q queue_name scriptname`. Например: `qsub -q big -l nodes=1 script.sh`. По умолчанию задачи попадают в очередь `eternity`.
- Для остановки/удаления задачи из очереди можно воспользоваться командой _qdel_:
`qdel 139843.head02`
- Для просмотра детальной информации о задачи можно использовать команду _qstat_:
`qstat -f 139843.head02`

#### Массивы задач

Когда нужно обработать много файлов, например, пробластовать их против базы, можно использовать массивы задач. Не делайте массивов больше чем на 1000 задач.
Например:

	#!/bin/bash
	#PBS -d .
	echo $PBS_ARRAYID

Запуск задачи: `qsub -t 0-39 start.sh`. Просмотр очереди: `qstat -t`.
Порождается 40 одинаковых задач, каждой выделяется свое ядро, каждой передается свое значение переменной `$PBS_ARRAYID` от 0 до 39. В зависимости от значения, можно внутри скрипта обратиться к какому-либо файлу, например `perl transl.pl $PBS_ARRAYID.fasta > $PBS_ARRAYID.prot`.

Можно ограничить ваш массив одним узлом: `#PBS -l nodes=node13`, тогда начнется выполнение первых 24-х задач, а остальные будут ждать в очереди.
Если указать строку `#PBS -l nodes=node13:ppn=20+node14:ppn=20` для массива задач, то эта строка работает не для всего массива, а для каждой задачи из массива. Таким образом, каждая из 100 задач будет пытаться занять 20 ядер на узле 13 и 20 ядер на узле 14. Следовательно, все считаться они будут медленно, и 39 из занятых 40 ядер будут простаивать. Не делайте так.

Типы доступных узлов (очереди):

| name     | cpu | mem  | node count | comment         |
|----------|-----|------|------------|-----------------|
| eternity | 24  | 48G  | 30         | default         |
| big      | 32  | 128G | 3          |                 |
| mem512g  | 48  | 512G | 1          |                 |
| mem1t    | 80  | 1T   | 1          |                 |
| mem1t    | 80  | 2T   | 1          |                 |
| gpu      | 24  | 48G  | 2          | Nvidia GTX 1060 |



### Ограничения по вводу-выводу

Все вычислительные узлы, а также рабочий узел _head02_ подключены к файл-серверу (/home). Если файл сервер начинает испытывать очень большие нагрузки по чтению-записи, то становится трудно работать на _head02_.

Не запускайте одновременно много задач интенсивно использующие I/O. Задачи будут тормозить значительно дольше, чем если бы они были бы запущены по очереди.

По умолчанию программы вроде _tar_, _cp_ запускаются с ionice -3. К сожалению, это нельзя сделать для _cat_. Если вам нужно объединить два огромных файла или что-то еще с помощью _cat_, то лучше используйте _pv_. Это интерактивный аналог _cat_, который может показывать _progress-bar_, скорость передачи данных, и самое главное, ограничивать скорость передачи данных подобно _rsync_. 
Пример использования: `pv -L 10m file1 file2 > concatenated`.

### Сборка геномов/транскриптомов

Для операций над большим количеством файлов, на узлах _node01_,_node02_ и _node36_,_node33-node35_ созданы специальные scratch разделы /mnt/scr размером 600Gb, 1Tb, 2Tb соответственно. Если вам необходимо, например, собрать транскриптом, то имеет смысл использовать именно их - это существенно поднимет скорость выполнения задачи.

Типичный PBS скрипт с использованием scratch директории будет выглядеть так:

	#!/bin/bash
	#PBS -d .

	# Copy data from main storage to scratch
	cp /home/my/data /mnt/scr/data
	cd /mnt/scr/data

	# Do some job
	do_some_job

	# Copy results back to main storage
	cp /mnt/scr/data/results /home/my/data

	# Clean all stuff
	rm -rf /mnt/scr/data

NB: Будьте внимательны, в случае Array job.

### Доступное ПО

Многие программы, необходимые для работы с геномными данными, уже установлены на кластере. Их можно найти в директории /home/tools. Если вам нужна какая-то программа, и её нет в /home/tools, но она может пригодиться другим, то установите её в /home/tools/newprogram или попросите об этом администраторов.

### Удаленный графический доступ

Мы запустили в тестовом режиме возможно удаленно работать с графическими приложениями. Все действия осуществляются на специально выделенном узле. В данный момент, по соображениям безопасности, доступ возможен только из сети Факультета.

Для доступа необходимо:
- Cкачать и установить клиент x2go под соответствующую ОС (поддерживаются GNU/Linux, Windows, Mac OS X)

- Согласно инструкции (вариант с картинками) создать новую сессию, где указать следующие параметры:

	Host: ma.fbb.msu.ru
	Login: your_username
	SSH port: 222
	Session type: GNOME

- Попробовать подключиться - все должно работать. Если вы используете для авторизации ключ, его также можно указать при создании сесии. На вкладке Connection можно выбрать тип вашего соединения, для оптимизации производительности. На вкладке Shared folders можно дополнительно подключить локальные директории (на вашем компьютере), которые вам будут также доступны на кластере. Соответственно можно легко копировать файлы, но этот интерфейс далеко не самый производительный. При отключении сессии сохраняются

### IGV

Одна из ключевых возможностей, ради которой мы реализовали удаленный графический доступ - возможность работы с геномными браузерами.

Для использования IGV необходимо открыть терминал и ввести следующие команды

	module load igv
	igv.sh

Первая активирует соответствующий модуль и устанавливает необходимые пути, вторая непосредственно запускает IGV.